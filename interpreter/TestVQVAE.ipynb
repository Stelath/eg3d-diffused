{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderKL\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = transforms.Resize(128)(image)\n",
    "image = transforms.ToTensor()(image).unsqueeze(0)\n",
    "model = AutoencoderKL().to('cuda')\n",
    "out = model.encode(image.to('cuda')).latent_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.5473e-01,  1.7594e+00,  1.3192e+00,  ...,  1.9767e-01,\n",
       "            1.5611e+00, -1.0620e+00],\n",
       "          [-1.1411e+00,  6.4325e-01,  3.2717e-01,  ..., -1.1717e+00,\n",
       "           -4.1933e-01,  2.9469e-01],\n",
       "          [-1.2109e+00, -8.6480e-01, -7.5517e-01,  ..., -8.9837e-01,\n",
       "           -3.2185e-01, -5.3190e-01],\n",
       "          ...,\n",
       "          [-2.2421e-01,  1.6826e+00,  7.2779e-01,  ...,  4.0762e-01,\n",
       "           -2.6253e-01, -6.4070e-01],\n",
       "          [-1.8365e+00, -4.4094e-01,  8.6749e-02,  ..., -9.6653e-01,\n",
       "           -5.5674e-01, -1.0174e+00],\n",
       "          [ 5.8822e-01,  7.1620e-02,  4.5480e-02,  ..., -7.5032e-01,\n",
       "           -1.4659e-01,  6.3315e-01]],\n",
       "\n",
       "         [[ 7.4479e-02, -1.4564e+00, -1.6481e+00,  ..., -7.8668e-01,\n",
       "            6.9922e-01,  4.8016e-01],\n",
       "          [ 8.5038e-04,  2.4102e-01,  1.4335e+00,  ..., -1.8494e-01,\n",
       "           -6.2167e-01,  2.8094e-01],\n",
       "          [-3.1910e-01, -1.4309e+00,  6.7293e-01,  ...,  6.6274e-01,\n",
       "           -7.0209e-01,  6.3646e-01],\n",
       "          ...,\n",
       "          [-1.0578e+00, -1.5662e+00, -6.1542e-01,  ..., -5.1303e-01,\n",
       "           -1.2330e+00, -1.2975e+00],\n",
       "          [-1.1447e+00,  9.8468e-01, -2.9193e+00,  ..., -1.1168e-01,\n",
       "           -1.0663e+00,  1.5613e-01],\n",
       "          [ 8.4813e-01, -2.7145e+00, -1.9501e+00,  ...,  1.2510e+00,\n",
       "            1.1906e-01, -1.4673e-01]],\n",
       "\n",
       "         [[ 1.4323e+00,  1.4837e+00,  1.6733e+00,  ...,  2.7317e+00,\n",
       "            5.0652e-01, -1.3242e+00],\n",
       "          [ 1.8344e+00,  5.6108e-01,  2.0781e+00,  ...,  5.7884e-01,\n",
       "           -5.3329e-02, -1.3148e+00],\n",
       "          [ 1.8833e+00, -1.2473e+00,  8.4900e-01,  ..., -4.0188e-01,\n",
       "            1.1400e+00,  7.6737e-01],\n",
       "          ...,\n",
       "          [-2.3517e-01, -1.6944e-01, -7.6695e-02,  ..., -2.9553e-02,\n",
       "           -1.1716e+00, -1.3746e+00],\n",
       "          [-1.1667e+00,  1.2673e+00,  1.1822e-02,  ..., -6.5613e-01,\n",
       "            3.6729e-01,  2.8643e-01],\n",
       "          [ 1.1177e-01, -3.7360e-02, -1.5918e+00,  ..., -7.8884e-01,\n",
       "           -2.1105e+00, -4.7284e-01]],\n",
       "\n",
       "         [[ 3.3223e-01,  3.0106e-01,  2.5503e-01,  ..., -1.4347e+00,\n",
       "           -8.1719e-01, -7.1911e-01],\n",
       "          [-6.6777e-02, -1.3292e+00, -6.3119e-01,  ..., -1.9302e+00,\n",
       "           -1.0311e+00, -1.7069e+00],\n",
       "          [-1.0945e+00,  3.3930e-01,  1.7445e+00,  ..., -2.2764e+00,\n",
       "            1.1083e+00,  4.9793e-01],\n",
       "          ...,\n",
       "          [-2.8138e+00,  1.2612e+00, -6.7562e-01,  ...,  1.3524e+00,\n",
       "            1.6284e+00,  8.2033e-01],\n",
       "          [-1.3345e+00, -1.2339e+00, -1.2575e+00,  ...,  5.1078e-01,\n",
       "           -9.3645e-01,  1.3520e-01],\n",
       "          [-2.0013e+00, -4.9115e-01,  1.3585e+00,  ...,  5.9612e-01,\n",
       "            6.4413e-01, -7.5082e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'logit_scale', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled (EOS token) states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  1125,   539,   320, 12464, 12464, 12464,  2368, 49407],\n",
       "        [49406,   320,  1125,   539,   320,  1929,   721, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"a photo of a dumb dumb dumb cat\", \"a photo of a dog today\"], padding=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 512])\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_state.shape)\n",
    "print(pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 12:41:18.603326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from eg3d_diffuser import EG3DConditional\n",
    "from diffusers import UNet1DModel\n",
    "\n",
    "diffuser = EG3DConditional(\n",
    "            sample_size=512,\n",
    "            in_channels=2,\n",
    "            out_channels=1,\n",
    "            layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "            block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "            time_embed_dim = 512,\n",
    "            down_block_types=(\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\", \"DownBlock1D\"),\n",
    "            up_block_types=(\"UpBlock1D\", \"AttnUpBlock1D\", \"UpBlock1D\", \"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet1DOutput(sample=tensor([[[-0.1018, -0.1018, -0.0593,  0.0032,  0.0856,  0.1181,  0.1004,\n",
       "           0.1107,  0.1490,  0.1412,  0.0874,  0.0725,  0.0968,  0.1290,\n",
       "           0.1690,  0.1959,  0.2097,  0.2231,  0.2363,  0.2599,  0.2941,\n",
       "           0.3170,  0.3286,  0.3381,  0.3456,  0.3495,  0.3498,  0.3435,\n",
       "           0.3306,  0.3242,  0.3243,  0.3277,  0.3345,  0.3414,  0.3484,\n",
       "           0.3542,  0.3586,  0.3611,  0.3615,  0.3614,  0.3607,  0.3608,\n",
       "           0.3617,  0.3626,  0.3637,  0.3635,  0.3620,  0.3624,  0.3649,\n",
       "           0.3693,  0.3754,  0.3823,  0.3900,  0.4004,  0.4137,  0.4282,\n",
       "           0.4439,  0.4609,  0.4790,  0.4971,  0.5150,  0.5321,  0.5484,\n",
       "           0.5608,  0.5695,  0.5746,  0.5761,  0.5781,  0.5805,  0.5819,\n",
       "           0.5823,  0.5815,  0.5796,  0.5782,  0.5774,  0.5760,  0.5741,\n",
       "           0.5726,  0.5716,  0.5720,  0.5739,  0.5750,  0.5755,  0.5764,\n",
       "           0.5775,  0.5789,  0.5806,  0.5833,  0.5871,  0.5931,  0.6014,\n",
       "           0.6105,  0.6205,  0.6310,  0.6420,  0.6546,  0.6687,  0.6832,\n",
       "           0.6981,  0.7142,  0.7314,  0.7499,  0.7697,  0.7899,  0.8102,\n",
       "           0.8306,  0.8511,  0.8710,  0.8905,  0.9084,  0.9249,  0.9396,\n",
       "           0.9523,  0.9642,  0.9753,  0.9850,  0.9932,  0.9996,  1.0042,\n",
       "           1.0075,  1.0097,  1.0105,  1.0099,  1.0087,  1.0068,  1.0054,\n",
       "           1.0046,  1.0037,  1.0027,  1.0008,  0.9982,  0.9947,  0.9903,\n",
       "           0.9857,  0.9808,  0.9751,  0.9687,  0.9616,  0.9538,  0.9459,\n",
       "           0.9379,  0.9295,  0.9207,  0.9116,  0.9022,  0.8933,  0.8846,\n",
       "           0.8759,  0.8671,  0.8584,  0.8499,  0.8417,  0.8338,  0.8261,\n",
       "           0.8186,  0.8112,  0.8039,  0.7965,  0.7889,  0.7817,  0.7747,\n",
       "           0.7683,  0.7622,  0.7565,  0.7511,  0.7458,  0.7407,  0.7360,\n",
       "           0.7317,  0.7279,  0.7246,  0.7215,  0.7185,  0.7157,  0.7132,\n",
       "           0.7107,  0.7082,  0.7056,  0.7029,  0.7003,  0.6980,  0.6956,\n",
       "           0.6933,  0.6910,  0.6885,  0.6863,  0.6841,  0.6821,  0.6801,\n",
       "           0.6784,  0.6767,  0.6752,  0.6737,  0.6723,  0.6709,  0.6696,\n",
       "           0.6683,  0.6672,  0.6664,  0.6656,  0.6648,  0.6640,  0.6631,\n",
       "           0.6624,  0.6619,  0.6615,  0.6611,  0.6607,  0.6605,  0.6604,\n",
       "           0.6605,  0.6607,  0.6609,  0.6613,  0.6619,  0.6627,  0.6637,\n",
       "           0.6647,  0.6658,  0.6670,  0.6682,  0.6696,  0.6712,  0.6729,\n",
       "           0.6748,  0.6769,  0.6790,  0.6813,  0.6837,  0.6862,  0.6887,\n",
       "           0.6912,  0.6938,  0.6965,  0.6994,  0.7024,  0.7056,  0.7090,\n",
       "           0.7127,  0.7167,  0.7210,  0.7257,  0.7307,  0.7360,  0.7415,\n",
       "           0.7472,  0.7533,  0.7595,  0.7659,  0.7728,  0.7801,  0.7877,\n",
       "           0.7958,  0.8042,  0.8128,  0.8217,  0.8308,  0.8402,  0.8497,\n",
       "           0.8595,  0.8693,  0.8794,  0.8897,  0.9002,  0.9107,  0.9214,\n",
       "           0.9321,  0.9430,  0.9539,  0.9648,  0.9757,  0.9867,  0.9975,\n",
       "           1.0083,  1.0188,  1.0293,  1.0396,  1.0498,  1.0599,  1.0698,\n",
       "           1.0795,  1.0889,  1.0981,  1.1070,  1.1156,  1.1240,  1.1322,\n",
       "           1.1403,  1.1482,  1.1560,  1.1637,  1.1714,  1.1793,  1.1871,\n",
       "           1.1949,  1.2026,  1.2105,  1.2182,  1.2259,  1.2334,  1.2405,\n",
       "           1.2472,  1.2537,  1.2596,  1.2650,  1.2701,  1.2748,  1.2792,\n",
       "           1.2832,  1.2867,  1.2897,  1.2925,  1.2951,  1.2974,  1.2994,\n",
       "           1.3012,  1.3029,  1.3043,  1.3055,  1.3065,  1.3071,  1.3077,\n",
       "           1.3082,  1.3083,  1.3079,  1.3071,  1.3059,  1.3042,  1.3021,\n",
       "           1.2996,  1.2969,  1.2941,  1.2913,  1.2885,  1.2857,  1.2831,\n",
       "           1.2806,  1.2783,  1.2761,  1.2741,  1.2723,  1.2710,  1.2701,\n",
       "           1.2698,  1.2699,  1.2702,  1.2706,  1.2713,  1.2722,  1.2732,\n",
       "           1.2744,  1.2755,  1.2766,  1.2776,  1.2785,  1.2795,  1.2808,\n",
       "           1.2821,  1.2834,  1.2848,  1.2863,  1.2880,  1.2901,  1.2919,\n",
       "           1.2936,  1.2947,  1.2954,  1.2957,  1.2957,  1.2948,  1.2930,\n",
       "           1.2905,  1.2874,  1.2834,  1.2787,  1.2732,  1.2670,  1.2597,\n",
       "           1.2513,  1.2420,  1.2317,  1.2216,  1.2116,  1.2014,  1.1910,\n",
       "           1.1804,  1.1696,  1.1584,  1.1469,  1.1349,  1.1223,  1.1084,\n",
       "           1.0933,  1.0777,  1.0615,  1.0455,  1.0296,  1.0131,  0.9960,\n",
       "           0.9793,  0.9629,  0.9470,  0.9316,  0.9158,  0.8995,  0.8851,\n",
       "           0.8724,  0.8612,  0.8516,  0.8427,  0.8344,  0.8266,  0.8191,\n",
       "           0.8108,  0.8015,  0.7915,  0.7807,  0.7702,  0.7601,  0.7513,\n",
       "           0.7439,  0.7367,  0.7297,  0.7234,  0.7177,  0.7128,  0.7089,\n",
       "           0.7050,  0.7013,  0.6978,  0.6945,  0.6901,  0.6848,  0.6783,\n",
       "           0.6708,  0.6620,  0.6518,  0.6400,  0.6268,  0.6117,  0.5946,\n",
       "           0.5780,  0.5618,  0.5462,  0.5313,  0.5175,  0.5050,  0.4948,\n",
       "           0.4869,  0.4807,  0.4761,  0.4737,  0.4736,  0.4762,  0.4818,\n",
       "           0.4902,  0.5013,  0.5154,  0.5325,  0.5497,  0.5670,  0.5823,\n",
       "           0.5957,  0.6060,  0.6133,  0.6166,  0.6161,  0.6131,  0.6077,\n",
       "           0.6015,  0.5947,  0.5885,  0.5831,  0.5779,  0.5730,  0.5677,\n",
       "           0.5620,  0.5559,  0.5493,  0.5407,  0.5300,  0.5205,  0.5120,\n",
       "           0.5025,  0.4919,  0.4709,  0.4394,  0.4037,  0.3637,  0.3306,\n",
       "           0.3043,  0.6192,  1.2752,  1.4618,  1.1789,  0.9350,  0.7302,\n",
       "           0.4893,  0.2122,  0.0711,  0.0662,  0.0438,  0.0040,  0.3310,\n",
       "           1.0249,  1.0377,  0.3694,  0.0419,  0.0552,  0.0501,  0.0266,\n",
       "           0.0266]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuser(torch.zeros((1, 2, 512)), 3, torch.zeros((1, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8052d1d792de26149c022630e24c9726f724881113f45b9acec04039870ce31b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
