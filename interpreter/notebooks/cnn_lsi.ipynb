{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a71927-535e-4eba-bfad-f4de5b384332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from eg3d_dataset import EG3DDataset\n",
    "from gen_samples import vision_evaluate\n",
    "from torchvision.models import convnext_base, convnext_small\n",
    "\n",
    "from eg3d_loss import EG3DLoss\n",
    "from eg3d import EG3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9eaf04-779e-4032-b1b9-2e45f3972429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 512  # the generated image resolution\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 8  # how many images to sample during evaluation\n",
    "    num_dataloader_workers = 12  # how many subprocesses to use for data loading\n",
    "    num_epochs = 60\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    scheduler_train_timesteps = 1000\n",
    "    eval_inference_steps = 1000\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 10\n",
    "    mixed_precision = 'no'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = 'eg3d-latent-interpreter'\n",
    "    \n",
    "    eg3d_model_path = 'eg3d/eg3d_model/ffhqrebalanced512-128.pkl'\n",
    "    data_dir = 'data_color/'\n",
    "    df_file = 'dataset.df'\n",
    "\n",
    "    overwrite_output_dir = True\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d36e9f-3311-4d3e-93b2-60fd4d2c659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = EG3DDataset(df_file=config.df_file, data_dir=config.data_dir, transform=preprocess, encode=False)\n",
    "\n",
    "train_size = int(len(dataset) * 0.95)\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=config.eval_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377276ae-268f-4262-b4e0-fb333e98122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_small(num_classes=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de957b6f-752b-49aa-ad67-5ed3a561186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "vector_loss_function = nn.SmoothL1Loss(reduction='mean')\n",
    "eg3d_loss_function = EG3DLoss(config.eg3d_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd75dd1d-db45-4de1-844a-c9c5cf28216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.199393664\n"
     ]
    }
   ],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "mem = mem_params + mem_bufs\n",
    "print(mem/(10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5772ddaa-33b0-4c43-bbf9-7e7b15cae5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, optimizer, vector_loss_function, eg3d_loss_function, train_dataloader, eval_dataloader):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"eg3d_latent_interpreter\")\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            images = batch['images']\n",
    "            latent_vectors = batch['latent_vectors']\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                latent_vectors_pred = model(images)\n",
    "                \n",
    "                loss = vector_loss_function(latent_vectors_pred, latent_vectors) + eg3d_loss_function(latent_vectors_pred, images)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                optimizer.step()\n",
    "                # lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"train_loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "        \n",
    "        model.eval()\n",
    "        avg_eval_loss = []\n",
    "        #for step, batch in enumerate(eval_dataloader):\n",
    "        batch = next(iter(eval_dataloader))\n",
    "        images = batch['images']\n",
    "        latent_vectors = batch['latent_vectors']\n",
    "        with torch.no_grad():\n",
    "            latent_vectors_pred = model(images)\n",
    "            \n",
    "            loss = vector_loss_function(latent_vectors_pred, latent_vectors) + eg3d_loss_function(latent_vectors_pred, images)\n",
    "            avg_eval_loss.append(loss.detach().item())\n",
    "        avg_eval_loss = sum(avg_eval_loss) / len(avg_eval_loss)\n",
    "        logs = {\"eval_loss\": avg_eval_loss}\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                render = epoch == config.num_epochs - 1\n",
    "                vision_evaluate(config, epoch, model, eval_dataloader, render=render)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, os.path.join(config.output_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f16784-9b70-48bb-9457-5f16714ebe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = (config, model, optimizer, vector_loss_function, eg3d_loss_function, train_dataloader, eval_dataloader)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017ce760-ecae-423e-b7ef-ac6b941f75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('eg3d-latent-interpreter/model.pth')['model_state_dict'])\n",
    "eg3d = EG3D('eg3d/eg3d_model/ffhqrebalanced512-128.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034affac-39a1-48cb-ad97-ffa5f7cb64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "batch = next(iter(eval_dataloader))\n",
    "imgs, targets = batch['images'], batch['latent_vectors']\n",
    "\n",
    "latent_vectors = model(transform(imgs))[0]\n",
    "\n",
    "print(targets[0][:10])\n",
    "print(latent_vectors[:10])\n",
    "print(np.std(targets.detach().cpu().numpy()))\n",
    "print(np.std(latent_vectors.detach().cpu().numpy()))\n",
    "\n",
    "img = eg3d.generate_imgs(latent_vectors.unsqueeze(0))[0].cpu().numpy()\n",
    "img = img.transpose(1, 2, 0)\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(imgs[0].cpu().numpy().transpose(1, 2, 0))\n",
    "axarr[1].imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4fc38-3f37-4cd2-ba46-cfa7a5945766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sample_dfs = sorted(glob.glob(f\"{config.output_dir}/samples/*.df\"))\n",
    "df = pd.read_pickle(sample_dfs[-1])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b1ba9-8e7c-4286-9b08-9d32b4aa2ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eg3d",
   "language": "python",
   "name": "eg3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
