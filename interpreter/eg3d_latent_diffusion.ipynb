{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08e1008-ce7c-4072-9499-7412eba2f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from eg3d_dataset import EG3DDataset\n",
    "from diffuser_utils.evaluate import vision_evaluate\n",
    "from torchvision.models import convnext_base, convnext_small\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from diffusers.models.vae import Encoder\n",
    "from diffusers import UNet1DModel\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "from eg3d_loss import EG3DLoss\n",
    "from eg3d import EG3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25e42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    rgb = True\n",
    "    image_size = 512  # the generated image resolution\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 8  # how many images to sample during evaluation\n",
    "    num_dataloader_workers = 12  # how many subprocesses to use for data loading\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    scheduler_train_timesteps = 30\n",
    "    eval_inference_steps = 30\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 10\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = 'eg3d-latent-diffusion'\n",
    "    \n",
    "    eg3d_model_path = 'eg3d/eg3d_model/ffhqrebalanced512-128.pkl'\n",
    "    eg3d_latent_vector_size = 512\n",
    "    \n",
    "    data_dir = 'data_color/'\n",
    "    df_file = 'dataset.df'\n",
    "\n",
    "    overwrite_output_dir = True\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = EG3DDataset(df_file=config.df_file, data_dir=config.data_dir, transform=preprocess, encode=False)\n",
    "\n",
    "train_size = int(len(dataset) * 0.95)\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=config.eval_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52574e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3 if config.rgb else 1\n",
    "encoder = Encoder(in_channels=1, out_channels=1)\n",
    "\n",
    "unetModel = UNet1DModel(\n",
    "    sample_size=config.eg3d_latent_vector_size,  # the target image resolution\n",
    "    in_channels=in_channels,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "    down_block_types=( \n",
    "        \"DownBlock1D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock1D\", \n",
    "        \"DownBlock1D\", \n",
    "        \"DownBlock1D\", \n",
    "        \"AttnDownBlock1D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock1D\",\n",
    "    ), \n",
    "    up_block_types=(\n",
    "        \"UpBlock1D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock1D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock1D\", \n",
    "        \"UpBlock1D\", \n",
    "        \"UpBlock1D\", \n",
    "        \"UpBlock1D\"  \n",
    "      ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=config.scheduler_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a69533",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "test_inp = torch.rand((1, 1, 512, 512))\n",
    "encoder = encoder\n",
    "outputs = encoder(test_inp)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abce592",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    encoder,\n",
    "    unetModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, loss_function, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"train_eg3d_diffuser\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the \n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = EG3DPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline.save_pretrained(config.output_dir) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) \n[Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d01b5a543a82692238f3084f96aae2e1edb23ab05d685e1cb1b514c5d096d7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
