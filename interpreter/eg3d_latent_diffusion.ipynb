{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08e1008-ce7c-4072-9499-7412eba2f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from eg3d_dataset import EG3DDataset\n",
    "from diffuser_utils.evaluate import evaluage_encoder, evaluate\n",
    "from torchvision.models import convnext_base, convnext_small\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from diffusers import UNet1DModel\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "from eg3d_pipeline import EG3DPipeline\n",
    "from eg3d_encoder import EG3DEncoder\n",
    "from eg3d_loss import EG3DLoss\n",
    "from eg3d import EG3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25e42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    rgb = True\n",
    "    image_size = 128  # the generated image resolution\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 8  # how many images to sample during evaluation\n",
    "    num_dataloader_workers = 12  # how many subprocesses to use for data loading\n",
    "    encoder_epochs = 200\n",
    "    diffuser_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    scheduler_train_timesteps = 30\n",
    "    eval_inference_steps = 30\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 10\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = 'eg3d-latent-diffusion'\n",
    "    \n",
    "    eg3d_model_path = 'eg3d/eg3d_model/ffhqrebalanced512-128.pkl'\n",
    "    eg3d_latent_vector_size = 512\n",
    "    \n",
    "    data_dir = 'data_color/'\n",
    "    df_file = 'dataset.df'\n",
    "\n",
    "    overwrite_output_dir = True\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = EG3DDataset(df_file=config.df_file, data_dir=config.data_dir, transform=preprocess, encode=False)\n",
    "\n",
    "train_size = int(len(dataset) * 0.95)\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52574e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg3d = EG3D(config.model_path)\n",
    "\n",
    "in_channels = 3 if config.rgb else 1\n",
    "encoder = EG3DEncoder(in_channels=3, out_channels=1)\n",
    "\n",
    "unetModel = UNet1DModel(\n",
    "    sample_size=config.eg3d_latent_vector_size,  # the target image resolution\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "    down_block_types=( \n",
    "        \"DownBlock1D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock1D\", \n",
    "        \"DownBlock1D\", \n",
    "        \"DownBlock1D\", \n",
    "        \"AttnDownBlock1D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock1D\",\n",
    "    ), \n",
    "    up_block_types=(\n",
    "        \"UpBlock1D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock1D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock1D\", \n",
    "        \"UpBlock1D\", \n",
    "        \"UpBlock1D\", \n",
    "        \"UpBlock1D\"  \n",
    "      ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0022e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = LMSDiscreteScheduler(num_train_timesteps=config.scheduler_train_timesteps)\n",
    "\n",
    "encoder_optimizer = torch.optim.AdamW(encoder.parameters(), lr=config.learning_rate)\n",
    "vector_loss_function = nn.SmoothL1Loss(reduction='mean')\n",
    "eg3d_loss_function = EG3DLoss(model=eg3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder_loop(config, model, optimizer, vector_loss_function, eg3d_loss_function, train_dataloader, eval_dataset):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"eg3d_li_encoder\")\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.encoder_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            images = batch['images']\n",
    "            latent_vectors = batch['latent_vectors']\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                latent_vectors_pred = model(images)\n",
    "                \n",
    "                loss = vector_loss_function(latent_vectors_pred, latent_vectors) + eg3d_loss_function(latent_vectors_pred, images)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                optimizer.step()\n",
    "                # lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"train_loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "        \n",
    "        model.eval()\n",
    "        avg_eval_loss = []\n",
    "        #for step, batch in enumerate(eval_dataloader):\n",
    "        batch = next(iter(eval_dataloader))\n",
    "        images = batch['images']\n",
    "        latent_vectors = batch['latent_vectors']\n",
    "        with torch.no_grad():\n",
    "            latent_vectors_pred = model(images)\n",
    "            \n",
    "            loss = vector_loss_function(latent_vectors_pred, latent_vectors) + eg3d_loss_function(latent_vectors_pred, images)\n",
    "            avg_eval_loss.append(loss.detach().item())\n",
    "        avg_eval_loss = sum(avg_eval_loss) / len(avg_eval_loss)\n",
    "        logs = {\"eval_loss\": avg_eval_loss}\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                render = epoch == config.num_epochs - 1\n",
    "                evaluate_encoder(config, epoch, model, eg3d_loss_function.get_eg3d(), eval_dataloader)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, os.path.join(config.output_dir, f'encoder_{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffuser_loop(config, model, noise_scheduler, optimizer, loss_function, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"eg3d_li_diffuser\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the \n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.diffuser_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = EG3DPipeline(encoder=encoder, unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline.save_pretrained(config.output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "# args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "args = (config, encoder, encoder_optimizer, vector_loss_function, eg3d_loss_function, train_dataloader, eval_dataset)\n",
    "\n",
    "notebook_launcher(encoder_train_loop, args, num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eg3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "028272c06e43467b8f692d99e1767743678ae503c811b1242e11a4c410f8c2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
