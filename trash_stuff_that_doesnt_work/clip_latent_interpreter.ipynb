{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a71927-535e-4eba-bfad-f4de5b384332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from eg3d_dataset import EG3DDataset, EG3DImageProcessor\n",
    "from gen_samples import vision_evaluate\n",
    "\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModel, CLIPVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9eaf04-779e-4032-b1b9-2e45f3972429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 512  # the generated image resolution\n",
    "    train_batch_size = 128\n",
    "    eval_batch_size = 128  # how many images to sample during evaluation\n",
    "    num_dataloader_workers = 12  # how many subprocesses to use for data loading\n",
    "    num_epochs = 60\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    scheduler_train_timesteps = 1000\n",
    "    eval_inference_steps = 1000\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'no'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = 'vision-eg3d-latent-interpreter'\n",
    "    \n",
    "    data_dir = 'data_color/'\n",
    "    df_file = 'dataset.df'\n",
    "\n",
    "    overwrite_output_dir = True\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d36e9f-3311-4d3e-93b2-60fd4d2c659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = EG3DImageProcessor()\n",
    "\n",
    "dataset = EG3DDataset(df_file=config.df_file, data_dir=config.data_dir, image_size=128, transform=preprocess, encode=False)\n",
    "\n",
    "train_size = int(len(dataset) * 0.95)\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=config.eval_batch_size, shuffle=True, num_workers=config.num_dataloader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377276ae-268f-4262-b4e0-fb333e98122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_config = CLIPVisionConfig(hidden_size=512, num_hidden_layers=32, num_attention_heads=32)\n",
    "model = CLIPVisionModel(vision_config)\n",
    "processor = CLIPImageProcessor(size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5c23e1-8c06-43f4-98d0-f6e0782bee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# inputs = processor(images=[dataset[0]['images'], dataset[1]['images']], return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# last_hidden_state = outputs.last_hidden_state\n",
    "# pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f544eeef-30d5-492d-a8de-1cfdff903f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de957b6f-752b-49aa-ad67-5ed3a561186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "loss_function = nn.SmoothL1Loss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd75dd1d-db45-4de1-844a-c9c5cf28216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5772ddaa-33b0-4c43-bbf9-7e7b15cae5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, processor, optimizer, train_dataloader, eval_dataloader):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"clip_latent_interpreter\")\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            images = batch['images']\n",
    "            latent_vectors = batch['latent_vectors']\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                latent_vectors_pred = model(pixel_values=images).pooler_output\n",
    "                \n",
    "                loss = loss_function(latent_vectors_pred, latent_vectors)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                optimizer.step()\n",
    "                # lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"train_loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "        \n",
    "        model.eval()\n",
    "        avg_eval_loss = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                latent_vectors_pred = model(pixel_values=images).pooler_output\n",
    "                \n",
    "                loss = loss_function(latent_vectors_pred, latent_vectors)\n",
    "                avg_eval_loss.append(loss.detach().item())\n",
    "        avg_eval_loss = sum(avg_eval_loss) / len(avg_eval_loss)\n",
    "        logs = {\"eval_loss\": avg_eval_loss}\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                render = epoch == config.num_epochs - 1\n",
    "                vision_evaluate(config, epoch, processor, model, eval_dataloader, render=render)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, os.path.join(config.output_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29f16784-9b70-48bb-9457-5f16714ebe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45fecf2213f47138662202fda31c919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e442289892f64861ac4d387ff01c12ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9f55f88f534e90bc0daddeadecef20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cca26130bf48be939e7093faf084b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5397e196e5e042c8b4ca9db97a595135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e476c826bb49fabe17f6e925d7b89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab1f0f96e0e4790ade3b333e980799d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6051636ad140e19b910f8f5ae65d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a7303e570e4d0e85fcb7da335eac8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e412c54367a74dc18c4b54cea077ab04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85040fe749e44d7a117cee4fe717c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c80b9a0f224989a3952cfaeea041f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m (config, model, processor, optimizer, train_dataloader, eval_dataloader)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/eg3d/lib/python3.9/site-packages/accelerate/launchers.py:135\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, use_fp16, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch_environment(use_mps_device\u001b[38;5;241m=\u001b[39muse_mps_device):\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(config, model, processor, optimizer, train_dataloader, eval_dataloader)\u001b[0m\n\u001b[1;32m     28\u001b[0m latent_vectors_pred \u001b[38;5;241m=\u001b[39m model(pixel_values\u001b[38;5;241m=\u001b[39mimages)\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(latent_vectors_pred, latent_vectors)\n\u001b[0;32m---> 31\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# lr_scheduler.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eg3d/lib/python3.9/site-packages/accelerate/accelerator.py:1316\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/eg3d/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/eg3d/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = (config, model, processor, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb4fc38-3f37-4cd2-ba46-cfa7a5945766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latent_vectors</th>\n",
       "      <th>latent_vectors_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.46562985, 1.334632, 1.3463336, -0.21872601...</td>\n",
       "      <td>[0.34967417, 0.33072296, -0.028596513, 0.03216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.29773626, 0.31208906, 1.0516183, -1.5730159...</td>\n",
       "      <td>[0.5148991, 0.08528648, 0.065244794, -0.153878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.6659024, 0.89474744, 2.2324452, -1.1894681,...</td>\n",
       "      <td>[0.32946944, 0.102209136, -0.16398558, -0.0494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.3788743, 2.4546614, 0.36312538, -2.1257122,...</td>\n",
       "      <td>[0.276742, 0.17762648, 0.034863807, -0.0208411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.31123757, 0.17900072, 1.3598925, -1.8150185...</td>\n",
       "      <td>[0.17728442, 0.33686158, -0.044201493, 0.02935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[1.2418977, -0.9253846, -0.13435465, -0.433270...</td>\n",
       "      <td>[0.07940714, 0.016563637, -0.10800985, -0.0164...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[-0.057110276, -0.43176275, -0.21717091, 0.219...</td>\n",
       "      <td>[0.2501511, 0.008996903, -0.17652346, -0.08992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[1.8535191, 0.035906322, -0.27818182, -1.50999...</td>\n",
       "      <td>[0.21476893, 0.0716762, -0.104455546, -0.10830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[-0.006629809, -0.6767008, -0.6187405, -0.5521...</td>\n",
       "      <td>[0.16533367, -0.07044939, -0.19103174, -0.1112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[-1.9417938, -0.08421868, -0.15561429, 0.19957...</td>\n",
       "      <td>[0.25613418, 0.09930204, -0.09046651, 0.033264...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        latent_vectors  \\\n",
       "0    [-0.46562985, 1.334632, 1.3463336, -0.21872601...   \n",
       "1    [0.29773626, 0.31208906, 1.0516183, -1.5730159...   \n",
       "2    [0.6659024, 0.89474744, 2.2324452, -1.1894681,...   \n",
       "3    [0.3788743, 2.4546614, 0.36312538, -2.1257122,...   \n",
       "4    [0.31123757, 0.17900072, 1.3598925, -1.8150185...   \n",
       "..                                                 ...   \n",
       "123  [1.2418977, -0.9253846, -0.13435465, -0.433270...   \n",
       "124  [-0.057110276, -0.43176275, -0.21717091, 0.219...   \n",
       "125  [1.8535191, 0.035906322, -0.27818182, -1.50999...   \n",
       "126  [-0.006629809, -0.6767008, -0.6187405, -0.5521...   \n",
       "127  [-1.9417938, -0.08421868, -0.15561429, 0.19957...   \n",
       "\n",
       "                                   latent_vectors_pred  \n",
       "0    [0.34967417, 0.33072296, -0.028596513, 0.03216...  \n",
       "1    [0.5148991, 0.08528648, 0.065244794, -0.153878...  \n",
       "2    [0.32946944, 0.102209136, -0.16398558, -0.0494...  \n",
       "3    [0.276742, 0.17762648, 0.034863807, -0.0208411...  \n",
       "4    [0.17728442, 0.33686158, -0.044201493, 0.02935...  \n",
       "..                                                 ...  \n",
       "123  [0.07940714, 0.016563637, -0.10800985, -0.0164...  \n",
       "124  [0.2501511, 0.008996903, -0.17652346, -0.08992...  \n",
       "125  [0.21476893, 0.0716762, -0.104455546, -0.10830...  \n",
       "126  [0.16533367, -0.07044939, -0.19103174, -0.1112...  \n",
       "127  [0.25613418, 0.09930204, -0.09046651, 0.033264...  \n",
       "\n",
       "[128 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sample_dfs = sorted(glob.glob(f\"{config.output_dir}/samples/*.df\"))\n",
    "df = pd.read_pickle(sample_dfs[-1])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b1ba9-8e7c-4286-9b08-9d32b4aa2ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eg3d",
   "language": "python",
   "name": "eg3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
